# -*- coding: utf-8 -*-
"""
Created on Tue Jun 27 21:11:25 2017

@author: Saurabh
"""

from __future__ import print_function

from imutils.video import VideoStream
import datetime
import argparse
import imutils
import time
import math
import cv2
import numpy as np
import time
from multiprocessing import Process, Queue


from imutils.video.pivideostream import PiVideoStream
from picamera import PiCamera
from picamera.array import PiRGBArray
from imutils.video import FPS

#////////////////////////////////////////////////////////////////////////////////////tracking functions and declarations////////////////////////////////////////////////////////////////////////////////////
ROI_points=[]
image = None
MS_input = []
list1 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
list2 = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]

global vs
global frame

def ROI_capture(event, u, v, flags, param):

	#capture Region of Interest(ROI) by mouse click
	#global MS_input, ROI_points, image

	#circle ROI points with red color and draw it on the image
	if MS_input and event == cv2.EVENT_LBUTTONDOWN and len(ROI_points) < 4:
		ROI_points.append((u, v))
		cv2.circle(image, (u, v), 4, (0, 0, 255), 2)
		#cv2.namedWindow("frame", cv2.WINDOW_NORMAL)
		#cv2.resizeWindow('frame', 1000,600)
		cv2.imshow("frame", image)


def sign(x):
	if x > 0:
		return 1.
	elif x < 0:
		return -1.
	elif x == 0:
		return 0.

# construct the argument parse and parse the arguments
ap = argparse.ArgumentParser()
ap.add_argument("-p", "--picamera", type=int, default=-1,
	help="whether or not the Raspberry Pi camera should be used")
args = vars(ap.parse_args())

# initialize the video stream and allow the cammera sensor to warmup
#vs = VideoStream(usePiCamera=args["picamera"] > 0).start()
vs = PiVideoStream().start()

time.sleep(2.0)
fps = FPS().start()
#global MS_input, ROI_points, image
count=0
(dX,dY) = (0,0)
#initialize counter to save frames
counter = 0

#capture video from camera
#frame = cv2.VideoCapture(0)

#call ROI_capture function to capture ROI points
cv2.namedWindow("frame")
cv2.setMouseCallback("frame", ROI_capture)

#termination criteria for camshift algorithm.
termination = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 5, 1)

#bounding box for roi
BBox_roi = None

#bounding box for refrence point
BBox_ref = None

#store previous angle for angle difference calculation
angle_buf = 0;
x2 = 0
y2 = 0

#///////////////////////////////////////////////////////////////////////////////////////looping starts///////////////////////////////////////////////////////////////////////////////////////////
# loop over the frames from the video stream
while True:
	# grab the frame from the threaded video stream and resize it
	# to have a maximum width of 400 pixels
	#frame = vs.read()
	#frame = imutils.resize(frame, width=300)
	frame = vs.read()
	frame = imutils.resize(frame, width=400)
	#capture the current frame
	#cap, image = frame
	#frame.read()
	image = frame
	#frame.release()
	#width = vs.get(3)
	#print(width)
	cv2.circle(image, (320,240), 180, (255, 0, 0), 2)
	#print(frame);
	#condition for frame capture by camera
	#check bounding box(ROI) is calculated or not
	if BBox_roi is not None:

	#convert frame from RGB to HSV space
		hsv_space = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
		Projection_back = cv2.calcBackProject([hsv_space], [0], BBox_hist, [0, 180], 1)

	#apply camshift algorithm for each ROI point
		(points, BBox_roi) = cv2.CamShift(Projection_back, BBox_roi, termination)
		pts = np.int32(cv2.boxPoints(points))

	#Draw bounding box around ROI
		cv2.polylines(image, [pts], True, (0, 255, 255), 2)

	#calculate centroid of ROI
		p0 = pts[0]
		p1 = pts[1]
		p2 = pts[2]
		p3 = pts[3]
		cx1 = (p0[0]+p3[0])/2
		cy1 = (p0[1]+p3[1])/2
		cx2 = (p1[0]+p2[0])/2
		cy2 = (p1[1]+p2[1])/2
		x1 = (cx1 + cx2)/2
		y1 = (cy1 + cy2)/2
		list1[count] = x1
		list2[count] = y1
		#print(x1,y1)
		if count<15:
			count = count+1
		if count==15:
			print(x1,y1)
			dX = list1[14] - list1[0]
			Cface[0]=list1[14];
			Cface[1]=list2[14];
			print(list1[14],list2[14])
			#print(list1)
			if abs(dX)>15:
				print(dX)
				print('east') if sign(dX) == -1 else print('West')
			#print(math.sqrt( (x2-x1)*(x2-x1) + (y2-y1)*(y2-y1) ) )
			count=0
			dY = list2[14]-list2[0]
			if abs(dY)>14:
				#print(dy)
				print('North') if sign(dY) == -1 else print('South')
				
			#if Cface[0] != 0:		# if the Center of the face is not zero (meaning no face was found)
			if Cface[0] > 180:	# The camera is moved diffrent distances and speeds depending on how far away-
				CamLeft(5,1)	#	from the center of that axis it detects a face
			if Cface[0] > 190:	#
				CamLeft(7,2)	#
			if Cface[0] > 200:	#
				CamLeft(9,3)	#
			
			if Cface[0] < 140:	# and diffrent dirrections depending on what side of center if finds a face.
				CamRight(5,1)
			if Cface[0] < 130:
				CamRight(7,2)
			if Cface[0] < 120:
				CamRight(9,3)
			
			if Cface[1] > 160:	# and moves diffrent servos depending on what axis we are talking about.
				CamDown(5,1)
			if Cface[1] > 170:
				CamDown(7,2)
			if Cface[1] > 190:
				CamDown(9,3)
			
			if Cface[1] < 140:
				CamUp(5,1)
			if Cface[1] < 130:
				CamUp(7,2)
			if Cface[1] < 110:
				CamUp(9,3)

		
		
		#display frame
		cv2.imshow("frame", image)
		file = "/home/ashutosh/image/frame"+ str(counter) + ".png"
		cv2.imwrite(file, image)
		counter = counter+1


		#press buttun for selecting reference point ROI
		button = cv2.waitKey(1) & 0xFF

		#press "q" to quit
		if button == ord("q"):
				break

	#display frame
	cv2.imshow("frame", image)
	fps.update()
	#press buttun for selecting actual ROI
	button = cv2.waitKey(1) & 0xFF

	#push "i" select reference point ROI
	if len(ROI_points) < 4 and button == ord("i"):

				#change status of mouse input and duplicate the frame
			MS_input = True
			orig = image.copy()

				#select ROI points until it reaches four
			while len(ROI_points) < 4:
					cv2.imshow("frame", image)
					cv2.waitKey(1)

				#create array and perform vertical addition
			ROI_points = np.array(ROI_points)
			add = ROI_points.sum(axis = 1)

				#select minimum and maximum point in Factual ROI
			lower = ROI_points[np.argmin(add)]
			upper = ROI_points[np.argmax(add)]

			#visit all the pixels under actual ROI and convert RGB to HSV
			region = orig[lower[1]:upper[1], lower[0]:upper[0]]
			region = cv2.cvtColor(region, cv2.COLOR_BGR2HSV)

			#calculate the current ROI histogram for future tracking
			BBox_hist = cv2.calcHist([region], [0], None, [16], [0, 180])
			BBox_hist = cv2.normalize(BBox_hist, BBox_hist, 0, 255, cv2.NORM_MINMAX)
			BBox_roi = (lower[0], lower[1], upper[0], upper[1])
			ROI_points = []

	#press "q" to quit
	elif button == ord("q"):
			break

#/////////////////////////////////////////////////////////////////////////////////////////loop ends///////////////////////////////////////////////////////////////////////////////////////////////////
#wipe out buffer
fps.stop()
print("[INFO] elasped time: {:.2f}".format(fps.elapsed()))
print("[INFO] approx. FPS: {:.2f}".format(fps.fps()))
frame.release()
cv2.destroyAllWindows()

# draw the timestamp on the frame
timestamp = datetime.datetime.now()
ts = timestamp.strftime("%A %d %B %Y %I:%M:%S%p")
cv2.putText(frame, ts, (10, frame.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX,
	0.35, (0, 0, 255), 1)

# show the frame
cv2.imshow("Frame", frame)
key = cv2.waitKey(1) & 0xFF

# if the `q` key was pressed, break from the loop
#if key == ord("q"):
#	break

#do a bit of cleanup
cv2.destroyAllWindows()
vs.stop()

     
